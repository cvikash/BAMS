{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from pandas import read_csv\n",
    "from scipy import stats\n",
    "\n",
    "window_size = 192\n",
    "stride_size = 24\n",
    "target_window_size = 24\n",
    "num_covariates = 3\n",
    "train_start = '2011-01-01 00:00:00'\n",
    "train_end = '2014-08-31 23:00:00'\n",
    "test_start = '2014-08-25 00:00:00' #need additional 7 days as given info\n",
    "test_end = '2014-09-07 23:00:00'\n",
    "\n",
    "def prep_data(data, covariates, data_start, train = True):\n",
    "    time_len = data.shape[0]\n",
    "    input_size = window_size-stride_size\n",
    "    windows_per_series = np.full((num_series), (time_len-input_size-target_window_size) // stride_size)\n",
    "    if train: windows_per_series -= (data_start+stride_size-1) // stride_size\n",
    "    total_windows = np.sum(windows_per_series)\n",
    "    x_input = np.zeros((total_windows, window_size, 1 + num_covariates), dtype='float32')\n",
    "    label = np.zeros((total_windows, target_window_size, 1 + num_covariates), dtype='float32')\n",
    "    v_input = np.zeros((total_windows, 2), dtype='float32')\n",
    "    count = 0\n",
    "    for series in trange(num_series): # for each time series\n",
    "        for i in range(windows_per_series[series]):\n",
    "            if train:\n",
    "                window_start = stride_size*i+data_start[series]\n",
    "            else:\n",
    "                window_start = stride_size*i\n",
    "            window_end = window_start+window_size\n",
    "            target_window_end = window_end+target_window_size\n",
    "            x_input[count, :, 0] = data[window_start:window_end, series]\n",
    "            x_input[count, :, 1:1+num_covariates] = covariates[window_start:window_end, :]\n",
    "            label[count, :, 0] = data[window_end:target_window_end, series]\n",
    "            label[count,:, 1:1+num_covariates] = covariates[window_end:target_window_end, :]\n",
    "            nonzero_sum = (x_input[count, 1:input_size, 0]!=0).sum()\n",
    "            if nonzero_sum == 0:\n",
    "                v_input[count, 0] = 0\n",
    "            else:\n",
    "                v_input[count, 0] = np.true_divide(x_input[count, :input_size, 0].sum(),nonzero_sum)+1\n",
    "                x_input[count, :, 0] = x_input[count, :, 0]/v_input[count, 0]\n",
    "                label[count, :, 0] = label[count, :, 0]/v_input[count, 0]\n",
    "            count += 1\n",
    "    return x_input, v_input, label\n",
    "\n",
    "def gen_covariates(times, num_covariates):\n",
    "    covariates = np.zeros((times.shape[0], num_covariates))\n",
    "    for i, input_time in enumerate(times):\n",
    "        covariates[i, 0] = input_time.weekday()\n",
    "        covariates[i, 1] = input_time.hour\n",
    "        covariates[i, 2] = input_time.month\n",
    "    return covariates[:, :num_covariates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'LD2011_2014.txt'\n",
    "save_name = 'elect'\n",
    "save_path = os.path.join('/home/vikash/data', save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'LD2011_2014.txt'\n",
    "save_name = 'elect'\n",
    "save_path = os.path.join('data', save_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "csv_path = os.path.join(save_path, name)\n",
    "if not os.path.exists(csv_path):\n",
    "    zipurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip'\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(save_path)\n",
    "\n",
    "data_frame = pd.read_csv(csv_path, sep=\";\", index_col=0, parse_dates=True, decimal=',')\n",
    "data_frame = data_frame.resample('1H',label = 'left',closed = 'right').sum()[train_start:test_end]\n",
    "data_frame.fillna(0, inplace=True) # (32304, 370)\n",
    "# generate covariates (has both train and test limits)\n",
    "covariates = gen_covariates(data_frame[train_start:test_end].index, num_covariates) # (32304, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dims = pd.DataFrame(covariates).nunique().tolist()\n",
    "train_data = data_frame[train_start:train_end]\n",
    "test_data = data_frame[test_start:test_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_target_df = pd.DataFrame(scaler.transform(train_data), index=train_data.index, columns=train_data.columns)\n",
    "test_target_df = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_target_df.values\n",
    "test_data = test_target_df.values\n",
    "data_start = (train_data!=0).argmax(axis=0) #find first nonzero value in each time series\n",
    "total_time = data_frame.shape[0] #32304\n",
    "num_series = data_frame.shape[1] #370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 370/370 [00:08<00:00, 42.24it/s]\n",
      "100%|██████████| 370/370 [00:00<00:00, 12310.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, v_train, y_train = prep_data(train_data, covariates, data_start)\n",
    "X_test, v_test, y_test = prep_data(test_data, covariates, data_start, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388731, 24, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.train_len = self.data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return time series sequence, current covariates, label sequence, future covariates\n",
    "        return (self.data[index,:,0], self.data[index,:,1:1+num_covariates], self.label[index,:,0], self.label[index,:,1:1+num_covariates])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, v, label):\n",
    "        self.data = data\n",
    "        self.v = v\n",
    "        self.label = label\n",
    "        self.test_len = self.data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return time series sequence, current covariates, normalizing stats, label sequence, future covariates\n",
    "        return (self.data[index,:,0], self.data[index,:,1:1+num_covariates], self.v[index], self.label[index,:,0], self.label[index,:,1:1+num_covariates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "\n",
    "train_set = TrainDataset(X_train, y_train)\n",
    "test_set = TestDataset(X_test, v_test, y_test)\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), sampler=RandomSampler(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d)\n",
    "        if weight_norm:\n",
    "            self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.dropout_fn(F.relu(self.conv1(x.float())))\n",
    "        out = self.dropout_fn(F.relu(self.conv2(out)))\n",
    "        \n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        return F.relu(out + residual[:,:,-out.shape[2]:])\n",
    "\n",
    "class FutureResidual(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(FutureResidual, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features),\n",
    "#                                  nn.BatchNorm1d(in_features),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(in_features=in_features, out_features=in_features),)\n",
    "#                                  nn.BatchNorm1d(in_features),)\n",
    "        \n",
    "    def forward(self, lag_x, x):\n",
    "        out = self.net(x.squeeze())\n",
    "        return F.relu(torch.cat((lag_x, out), dim=2))\n",
    "\n",
    "class DeepTCN(nn.Module):\n",
    "    def __init__(self, cov_dims=cov_dims, num_class=num_series, embedding_dim=20, dilations=[1,2,4,8,16,24,32], p=0.25, device=torch.device('cuda')):\n",
    "        super(DeepTCN, self).__init__()\n",
    "        self.input_dim, self.cov_dims, self.embeddings, self.device = 1+(len(cov_dims)*embedding_dim), cov_dims, [], device\n",
    "        for cov in cov_dims:\n",
    "            self.embeddings.append(nn.Embedding(num_class, embedding_dim, device=device))\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for d in dilations:\n",
    "            self.encoder.append(ResidualBlock(input_dim=self.input_dim, num_filters=self.input_dim, d=d))\n",
    "        self.decoder = FutureResidual(in_features=self.input_dim-1)\n",
    "        self.mlp = nn.Sequential(nn.Linear(1158, 8), nn.BatchNorm1d(8), nn.SiLU(), nn.Dropout(p), nn.Linear(8,1), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x, current_cov, next_cov):\n",
    "        current_cov_embeddings, next_cov_embeddings = [], []\n",
    "        for cov_idx, cov_dim in enumerate(self.cov_dims):\n",
    "            current_cov_embeddings.append(self.embeddings[cov_idx](current_cov[:,:,cov_idx].to(self.device).long()))\n",
    "            next_cov_embeddings.append(self.embeddings[cov_idx](next_cov[:,:,cov_idx].to(self.device).long()))\n",
    "        embed_concat = torch.cat(current_cov_embeddings, dim=2).to(self.device)\n",
    "        next_cov_concat = torch.cat(next_cov_embeddings, dim=2).to(self.device)\n",
    "        \n",
    "        encoder_input = torch.cat((x.unsqueeze(2), embed_concat), dim=2)\n",
    "        encoder_input = encoder_input.permute(0, 2, 1)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            encoder_input = layer(encoder_input)\n",
    "        encoder_output = encoder_input.permute(0, 2, 1)\n",
    "        encoder_output = torch.reshape(encoder_output, (encoder_output.shape[0], 1, -1))\n",
    "        encoder_output = torch.repeat_interleave(encoder_output, next_cov_concat.shape[1], dim=1)\n",
    "\n",
    "        decoder_output = self.decoder(lag_x=encoder_output, x=next_cov_concat)\n",
    "        t, n = decoder_output.size(0), decoder_output.size(1)\n",
    "        decoder_output = decoder_output.view(t * n, -1)\n",
    "        output = self.mlp(decoder_output.float())\n",
    "        output = output.view(t, n, -1)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T07:37:10.616988Z",
     "iopub.status.busy": "2023-01-20T07:37:10.616272Z",
     "iopub.status.idle": "2023-01-20T07:37:10.633893Z",
     "shell.execute_reply": "2023-01-20T07:37:10.632378Z",
     "shell.execute_reply.started": "2023-01-20T07:37:10.616948Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "def train(model, device=torch.device('cuda'), num_epochs = 1, learning_rate = 1e-3):\n",
    "    train_len = len(train_loader)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_summary = np.zeros((train_len * num_epochs))\n",
    "    loss_fn = F.mse_loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_epoch = np.zeros(len(train_loader))\n",
    "\n",
    "        pbar = tqdm(train_loader)\n",
    "        for (ts_data_batch, current_covs_batch, labels_batch, next_covs_batch) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = torch.zeros(1, device=device, dtype=torch.float32)\n",
    "            out = model(ts_data_batch.to(device), current_covs_batch.to(device), next_covs_batch.to(device))\n",
    "            loss = loss_fn(out.float(), labels_batch.squeeze().to(device).float())\n",
    "            \n",
    "            pbar.set_description(f\"Loss:{loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_summary[epoch * train_len:(epoch + 1) * train_len] = loss.cpu().detach()\n",
    "        \n",
    "    return loss_summary, optimizer\n",
    "\n",
    "def evaluate(model, optimizer, device=torch.device('cuda')):\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_epoch = np.zeros(len(train_loader))\n",
    "\n",
    "        pbar = tqdm(test_loader)\n",
    "        for (ts_data_batch, current_covs_batch, v_batch, labels_batch, next_covs_batch) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(ts_data_batch.to(device), current_covs_batch.to(device), next_covs_batch.to(device))\n",
    "            results.append(out.squeeze(0).cpu())\n",
    "\n",
    "    predictions = torch.cat(results)\n",
    "    criterion = nn.MSELoss()\n",
    "    test_rmse = torch.sqrt(criterion(predictions, labels_batch)).item()\n",
    "    return test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T07:37:10.636487Z",
     "iopub.status.busy": "2023-01-20T07:37:10.635832Z",
     "iopub.status.idle": "2023-01-20T07:37:13.419875Z",
     "shell.execute_reply": "2023-01-20T07:37:13.418903Z",
     "shell.execute_reply.started": "2023-01-20T07:37:10.636401Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DeepTCN(device=torch.device('cuda')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T07:37:13.421649Z",
     "iopub.status.busy": "2023-01-20T07:37:13.421294Z",
     "iopub.status.idle": "2023-01-20T08:03:21.250009Z",
     "shell.execute_reply": "2023-01-20T08:03:21.248503Z",
     "shell.execute_reply.started": "2023-01-20T07:37:13.421615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:0.00854562409222126: 100%|██████████| 48591/48591 [13:06<00:00, 61.82it/s]   \n",
      "Loss:0.009649071842432022: 100%|██████████| 48591/48591 [13:01<00:00, 62.16it/s]  \n"
     ]
    }
   ],
   "source": [
    "loss, optimizer = train(model, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T08:03:21.251925Z",
     "iopub.status.busy": "2023-01-20T08:03:21.251489Z",
     "iopub.status.idle": "2023-01-20T08:03:21.379050Z",
     "shell.execute_reply": "2023-01-20T08:03:21.377886Z",
     "shell.execute_reply.started": "2023-01-20T08:03:21.251888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15075178444385529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, optimizer, device=torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
